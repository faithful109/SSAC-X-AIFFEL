{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. 카메라 스티커앱을 개선하자!  \n",
    "## 14-1.들어가며\n",
    "우리가 이전에 만들었던 카메라 스티커앱으로 동영상을 처리하도록 개선하는 프로젝트진행.  \n",
    "동영상은 단순이미지 처리하는 것 잉사응로 고려해야할 것이 많습니다.(피사체 움직임에 따른 예외상황, 처리속도, 화면떨림)  \n",
    "이번시간목표는\n",
    "  1. 동영상 쉽게 다룰 수 있는 방법 익히기\n",
    "  2. 스티커앱 성능 분석하기 + 간단한 개선 방법\n",
    "  3. 원하는 스펙을 수치로 지정하기\n",
    "  4. 칼만 필터로 동영상 성능 개선하기  \n",
    "  \n",
    "## 준비물\n",
    "프로젝트 디렉토리 생성\n",
    "```\n",
    "$ mkdir -p ~/aiffel/video_sticker_app/models\n",
    "$ mkdir -p ~/aiffel/video_sticker_app/images\n",
    "```\n",
    "파일 다운로드  \n",
    "```\n",
    "$ cd ~/aiffel/video_sticker_app\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/new_video_sticker_app.zip\n",
    "$ unzip new_video_sticker_app.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-2. 프로젝트(1) moviepy로 비디오 처리하기  \n",
    "우리가 쓸 동영상 처리 라이브러리는 파이썬 기반의 moviepy로 아래에서 moviepy가 성능면에서 적합한지 확인해보자  \n",
    "**1. moviepy를 통해서 주피터노트북상에서 비디오를 읽고 쓰는 프로그램을 작성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoClip, VideoFileClip\n",
    "from moviepy.editor import ipython_display\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플로 받은 video2.mp4를 moviepy로 읽고 width=640으로 축소해서 화면에 재생시키고, 그 영상을 mvpyresult.mp4파일로 저장.  \n",
    "저장한 후에 2개의 파일을 열어 두개의 화면크기, 파일사이즈 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/404 [00:00<?, ?it/s, now=None]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ssac19/aiffel/video_sticker_app/images/mvpyresult.mp4.\n",
      "MoviePy - Writing audio in mvpyresultTEMP_MPY_wvf_snd.mp3\n",
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/ssac19/aiffel/video_sticker_app/images/mvpyresult.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ssac19/aiffel/video_sticker_app/images/mvpyresult.mp4\n"
     ]
    }
   ],
   "source": [
    "# 읽기\n",
    "video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/video2.mp4'\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "clip.ipython_display(fps=30, loop=True, autoplay=True, rd_kwargs=dict(logger=None))\n",
    "\n",
    "# 쓰기\n",
    "result_video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/mvpyresult.mp4'\n",
    "clip.write_videofile(result_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. moviepy로 읽은 동영상을 numpy 형태로 변환하고 영상 밝기를 50% 어둡게 만든후 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  12%|█▏        | 48/403 [00:00<00:00, 452.98it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ssac19/aiffel/video_sticker_app/images/mvpyresult2.mp4.\n",
      "Moviepy - Writing video /home/ssac19/aiffel/video_sticker_app/images/mvpyresult2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ssac19/aiffel/video_sticker_app/images/mvpyresult2.mp4\n"
     ]
    }
   ],
   "source": [
    "# 읽기\n",
    "video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/video2.mp4'\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "clip.ipython_display(fps=30, loop=True, autoplay=True, rd_kwargs=dict(logger=None))\n",
    "\n",
    "# clip 에서 numpy 로 데이터 추출\n",
    "vlen = int(clip.duration*clip.fps)\n",
    "video_container = np.zeros((vlen, clip.size[1], clip.size[0], 3), dtype=np.uint8)\n",
    "for i in range(vlen):\n",
    "    img = clip.get_frame(i/clip.fps)\n",
    "    video_container[i] = (img * 0.5).astype(np.uint8)\n",
    "\n",
    "# 새 clip 만들기\n",
    "dur = vlen / clip.fps\n",
    "outclip = VideoClip(lambda t: video_container[int(round(t*clip.fps))], duration=dur)\n",
    "\n",
    "# 쓰기\n",
    "result_video_path2 = os.getenv('HOME')+'/aiffel/video_sticker_app/images/mvpyresult2.mp4'\n",
    "outclip.write_videofile(result_video_path2, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**영상을 읽고 쓰는데 필요한 시간 측정, OpenCV와 비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  12%|█▏        | 48/403 [00:00<00:00, 461.34it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ssac19/aiffel/video_sticker_app/images/mvpyresult.mp4.\n",
      "Moviepy - Writing video /home/ssac19/aiffel/video_sticker_app/images/mvpyresult.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ssac19/aiffel/video_sticker_app/images/mvpyresult.mp4\n",
      "[INFO] moviepy time : 2.51ms\n"
     ]
    }
   ],
   "source": [
    "# CASE 1 : moviepy 사용\n",
    "start = cv2.getTickCount()\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "\n",
    "vlen = int(clip.duration*clip.fps)\n",
    "video_container = np.zeros((vlen, clip.size[1], clip.size[0], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(vlen):\n",
    "    img = clip.get_frame(i/clip.fps)\n",
    "    video_container[i] = (img * 0.5).astype(np.uint8)\n",
    "\n",
    "dur = vlen / clip.fps\n",
    "outclip = VideoClip(lambda t: video_container[int(round(t*clip.fps))], duration=dur)\n",
    "\n",
    "mvpy_video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/mvpyresult.mp4'\n",
    "outclip.write_videofile(mvpy_video_path, fps=30)\n",
    "\n",
    "time = (cv2.getTickCount() - start) / cv2.getTickFrequency()\n",
    "print (f'[INFO] moviepy time : {time:.2f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cv time : 1.58ms\n"
     ]
    }
   ],
   "source": [
    "# CASE 2 : OpenCV 사용\n",
    "start = cv2.getTickCount()\n",
    "vc = cv2.VideoCapture(video_path)\n",
    "\n",
    "cv_video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/cvresult.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vw = cv2.VideoWriter(cv_video_path, fourcc, 30, (640,360))\n",
    "\n",
    "vlen = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "for i in range(vlen):\n",
    "    ret, img = vc.read()\n",
    "    if ret == False: break\n",
    "    \n",
    "    img_result = cv2.resize(img, (640, 360)) * 0.5\n",
    "    vw.write(img_result.astype(np.uint8))\n",
    "    \n",
    "time = (cv2.getTickCount() - start) / cv2.getTickFrequency()\n",
    "print (f'[INFO] cv time : {time:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. moviepy 를 이용할 때의 장단점을 분석해 봅시다. 주피터 노트북에 답변을 작성해 코드와 함께 제출해 주세요.**  \n",
    "ANS) moviepy는 numpy와 호환이 되며 간단하고 직관적인 면이 장점인것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-3. 프로젝트(2) 어디까지 만들고 싶은지 정의하기  \n",
    "### 1. 실시간 카메라 스티커앱을 만들어 봅시다\n",
    "이전노드에서 img2sticker_orig라는 메소드를 구현하여 addsticker.py에 저장.  \n",
    "해당 메소드를 그대로 복사해서 img2sticker라는 메소드를 만들고, newaddsticker.py에 저장.  \n",
    "오늘은 이 메소드를 보완, 구현하는 방향으로 진행  \n",
    "\n",
    "**CASE 1: 웹캠입력을 사용하는 경우**  \n",
    "cv2.VideoCapture(0)으로 웹캠 입력을 받아온다.\n",
    "현재의 기본적인 img2sticker 를 활용하여 가장 기본적인 웹캠 실시간 스티커앱을 아래와 같이 빠르게 만들어 보겠습니다.\n",
    "위에서 0은 시스템에 연결된 영상 입력장치의 인덱스로 보통 캡처할 장치가 하나(웹캠)이기에 0을 사용한다.  \n",
    "```\n",
    "#webcam_sticker.py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "from newaddsticker import img2sticker\n",
    "\n",
    "detector_hog = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./models/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def main():\n",
    "    cv2.namedWindow('show', 0)\n",
    "    cv2.resizeWindow('show', 640, 360)\n",
    "\n",
    "    vc = cv2.VideoCapture(0)   # 연결된 영상 장치의 인덱스, 하나만 있는 경우 0을 사용\n",
    "    img_sticker = cv2.imread('./images/king.png')\n",
    "\n",
    "    vlen = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print (vlen) # 웹캠은 video length 가 0 입니다.\n",
    "\n",
    "    # 정해진 길이가 없기 때문에 while 을 주로 사용합니다.\n",
    "    # for i in range(vlen):\n",
    "    while True:\n",
    "        ret, img = vc.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        start = cv2.getTickCount()\n",
    "        img = cv2.flip(img, 1)  # 보통 웹캠은 좌우 반전\n",
    "\n",
    "        # 스티커 메소드를 사용\n",
    "        img_result = img2sticker(img, img_sticker.copy(), detector_hog, landmark_predictor)   \n",
    "\n",
    "        time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
    "        print ('[INFO] time: %.2fms'%time)\n",
    "\n",
    "        cv2.imshow('show', img_result)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "아래 코드로 실행  \n",
    "```\n",
    "$ cd ~/aiffel/video_sticker_app && python webcam_sticker.py\n",
    "```  \n",
    "\n",
    "**CASE 2: 스마트폰 영상의 스트리밍 입력을 사용하는 경우**  \n",
    "우리는 OpenCV로 처리할 수 있는 [RTMP](https://juyoung-1008.tistory.com/30)를 활용  \n",
    "스마트폰에 RTMP 어플 설치하고 송출할 url을 지정하고 송출.  \n",
    "스트리밍을 할 경우는  \n",
    "```\n",
    "#webcam_sticker.py의 cv2.VideoCapture()을 수정    \n",
    "vc = cv2.VideoCapture('rtmp://rtmp.streamaxia.com/streamaxia/XXXXXX')   # XXXXXX 부분은 본인 어플리케이션에서 확인한 코드로 대체해 주세요. \n",
    "```\n",
    "다시 실행해보자  \n",
    "```\n",
    "$ cd ~/aiffel/video_sticker_app && python webcam_sticker.py\n",
    "```  \n",
    "### 2. 스티커앱을 실행하고 카메라를 고정하고 서서히 멀어져봅니다. 혹은 아주 가까이 다가가 봅니다. 얼굴을 찾지 못하는 거리를 기록해주세요.  \n",
    "일반적으로 약 15cm ~ 1m 30cm 범위 사이에서 얼굴 인식이 가능하다고 합니다. 실제로 측정했을 때 어떠한지 결과를 기록해 주세요.\n",
    "ANS) 15cm ~ 50cm  \n",
    "### 3. 다시 자리로 돌아온 후 고개를 상하좌우로 움직여주세요. yaw, pitch, roll 각도의 개념을 직접 실험해 보고 각각 몇 도까지 정상적으로 스티커앱이 동작하는지 기록해주세요.  \n",
    "(예시)\n",
    "yaw : y축 기준 회전 → 높이 축(-45 ~ 45도)\n",
    "picth : x축 기준 회전 → 좌우 축(-20 ~ 30도)\n",
    "roll : z축 기준 회전 → 거리 축(-45 ~ 45도)  \n",
    "실제 측정한 결과  \n",
    "yaw = -45 ~ 45  \n",
    "pitch = -30 ~ 30  \n",
    "roll = -45 ~ 45  \n",
    "### 4. 만들고 싶은 스티커앱의 스펙(허용 거리, 허용 인원 수, 허용 각도, 안정성)을 정해주세요.  \n",
    "(예시)\n",
    "거리 : 25cm ~ 1m → 너무 가까우면 스티커의 의미가 없음, 셀카봉을 들었을 때의 유효거리\n",
    "인원 수 : 4명 → 4인 가족 기준\n",
    "허용 각도 : pitch : -20 ~ 30도, yaw : -45 ~ 45도, roll : -45 ~ 45도 → 화면을 바라볼 수 있는 각도\n",
    "안정성 : 위 조건을 만족하면서 FPPI (false positive per image) 기준 < 0.003, MR (miss rate) < 1 300장당 1번 에러 = 10초=30X10에 1번 에러  \n",
    "**기준의 이유를 어떻게 정했는지가 중요합니다. → 서비스 관점, 엔지니어링 관점으로 설명하면 좋습니다.**  \n",
    "ANS) 원하는 스펙.\n",
    "1. 거리: 요즘 핸드폰은 광각이기에 셀카봉이 없어도 된다고 판단하여 사람이 팔 길이 허용범위인 15 ~ 50cm\n",
    "2. 인원: 사람팔과 광각일경우 가로로 찍으면 4명정도가 가능한 걸로 알고 있습니다.\n",
    "3. 각도: yaw: =-60(고개를 돌린 경우가 많기 때문에), pitch= -10 ~ +30(보통 사진이 잘 안나오는 아래쪽에서의 셀카는 잘찍지않는다.), roll: =-30 보통 30도 내외에서 찍는 경우가 많아보입니다.\n",
    "4. 안정성: 보통 핸드폰의 60hz인데, 60번당 0.5번정도 에러나는 안정성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"webcam_sticker.py\", line 43, in <module>\n",
    "    main()\n",
    "  File \"webcam_sticker.py\", line 31, in main\n",
    "    img_result = img2sticker(img, img_sticker.copy(), detector_hog, landmark_predictor)   \n",
    "  File \"/home/ssac19/aiffel/video_sticker_app/newaddsticker.py\", line 42, in img2sticker\n",
    "    cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "cv2.error: OpenCV(4.5.1) /tmp/pip-req-build-hj027r8z/opencv/modules/core/src/arithm.cpp:666: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
    "```\n",
    "원하는 스펙.\n",
    "1. 거리: 요즘 핸드폰은 광각이기에 셀카봉이 없어도 된다고 판단하여 사람이 팔 길이 허용범위인 15 ~ 50cm\n",
    "2. 인원: 사람팔과 광각일경우 가로로 찍으면 4명정도가 가능한 걸로 알고 있습니다.\n",
    "3. 각도: yaw: =-60(고개를 돌린 경우가 많기 때문에), pitch= -10 ~ +30(보통 사진이 잘 안나오는 아래쪽에서의 셀카는 잘찍지않는다.), roll: =-30 보통 30도 내외에서 찍는 경우가 많아보입니다.\n",
    "4. 안정성: 보통 핸드폰의 60hz인데, 60번당 0.5번정도 에러나는 안정성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹캠과 얼굴이 너무 멀어져서 얼굴인식이 안되는 경우.\n",
    "yaw를 +-30도 초과시에 에러."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-4. 프로젝트 (3) 스티커 Out Bound 예외처리하기  \n",
    "이전 스텝의 기본 스티커앱의 문제점을 보완해보자.\n",
    "\n",
    "### 1. 지금까지 만든 스티커앱을 이용해서 예외 상황을 찾아주세요. 특히 서서히 영상에서 좌우 경계 밖으로 나가며 코드의 행동을 확인해 보세요.  \n",
    "ANS) 웹캠과 얼굴이 너무 멀어져서 얼굴인식이 안되는 경우, yaw를 +-30도 초과시에 에러.  \n",
    "\n",
    "\n",
    "### 2. 문제가 어디에서 발생하는지 코드에서 확인합니다.  \n",
    "(힌트) 얼굴이 카메라 왼쪽 경계를 벗어나서 detection 되는 경우 refined_x 의 값이 음수가 됩니다.\n",
    "img_bgr[..., refined_x:...] 에서 numpy array의 음수 index에 접근하게 되므로 예외가 발생합니다.\n",
    "newaddsticker.py의 img2sticker 메소드에서 아래 부분을 수정해 주어야 합니다.\n",
    "```\n",
    "### (이전 생략) ###\n",
    "\n",
    "# sticker\n",
    "img_sticker = cv2.resize(img_sticker, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "refined_x = x - w // 2\n",
    "refined_y = y - h\n",
    "\n",
    "if refined_y < 0:\n",
    "    img_sticker = img_sticker[-refined_y:]\n",
    "    refined_y = 0\n",
    "\n",
    "###\n",
    "# TODO : x 축 예외처리 코드 추가\n",
    "###\n",
    "\n",
    "img_bgr = img_orig.copy()\n",
    "sticker_area = img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "\n",
    "img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]] = \\\n",
    "    cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "\n",
    "return img_bgr\n",
    "```\n",
    "-------------------------------------------------------\n",
    "### 3. Out bound 오류(경계 밖으로 대상이 나가서 생기는 오류)를 해결해 주세요.  \n",
    "위 예외처리 코드 부분에 들어가야 하는 코드는 아래와 같습니다. newaddsticker.py 파일을 수정해 주세요.  \n",
    "```\n",
    "    if refined_x < 0:\n",
    "        img_sticker = img_sticker[:, -refined_x:]\n",
    "        refined_x = 0\n",
    "    elif refined_x + img_sticker.shape[1] >= img_orig.shape[1]:\n",
    "        img_sticker = img_sticker[:, :-(img_sticker.shape[1]+refined_x-img_orig.shape[1])]\n",
    "```\n",
    "\n",
    "### 4. 다른 예외는 어떤 것들이 있는지 정의해 주세요. 어떤 것이 문제가 되는지 스스로 정해봅시다.  \n",
    "꼼꼼이 찾아보면 위에서 수정한 것과 같은 명백한 오류 발생 케이스 이외에도 다양한 예외 상황을 찾아볼 수 있을 것입니다.\n",
    "\n",
    "정확한 정답은 없습니다. 본인이 정의하기 나름입니다.\n",
    "저는 '고개를 왼쪽아래로 향하게 할 때 스티커의 모양이 일정한 것' 을 예외 상황이라고 정의했습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-5. 프로젝트 (4) 스티커앱 분석 - 거리, 인원 수, 각도, 시계열 안정성  \n",
    "### 1. 멀어지는 경우에 왜 스티커앱이 동작하지 않는지 분석해주세요. detection, landmark, blending 단계 중 무엇이 문제일까요?\n",
    "dlib detection 이 문제입니다. 멀어지면 detector_hog 단계에서 bbox 가 출력되지 않습니다.  \n",
    "```\n",
    "# preprocess\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    # detector\n",
    "    img_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 0)\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "```  \n",
    "\n",
    "\n",
    "\n",
    "### 2. detector_hog의 문제를 해결하기 위해, 이미지 피라미드를 조절하여 성능을 향상시키는 간단한 방법이 있습니다. 이 방법을 활용하여 img2sticker 메소드를 간단히 고쳐 봅시다.  \n",
    "```\n",
    "def img2sticker(img_orig, img_sticker, detector_hog, landmark_predictor):\n",
    "    # preprocess\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # detector\n",
    "    img_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 1) # <- 이미지 피라미드 수 변경\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "\n",
    "    # landmark\n",
    "    list_landmarks = []\n",
    "    for dlib_rect in dlib_rects:\n",
    "        points = landmark_predictor(img_rgb_vga, dlib_rect)\n",
    "        list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        list_landmarks.append(list_points)\n",
    "```\n",
    "수정 후에 webcam_sticker.py 를 다시한번 실행하여 스티커앱이 잘 실행되는지 확인해 봅시다.\n",
    "\n",
    "### 3. 위에서 새롭게 시도한 방법의 문제점은 무엇인가요?\n",
    "속도가 현저히 느려집니다. 기존 30ms/frame 에서 120ms/frame 으로 약 4배 느려짐 → 실시간 구동이 불가능 합니다.\n",
    "\n",
    "### 4. 실행시간을 만족할 수 있는 방법을 찾아봅시다.\n",
    "hog 디텍터 -> 딥러닝 기반 디텍터로 변경할 수 있습니다. hog 학습 단계에서 다양한 각도에 대한 hog 특징을 모두 추출해서 일반화 하기 어렵기 때문에 딥러닝 기반 검출기의 성능이 훨씬 좋습니다.\n",
    "\n",
    "딥러닝 기반 detection 방법을 조사합니다. 아래 링크를 참고하면 도움이 될 것입니다.\n",
    "[How does the OpenCV deep learning face detector work?](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)\n",
    "\n",
    "opencv 는 intel cpu 을 사용할 때 dnn 모듈이 가속화를 지원하고 있습니다. 따라서 mobilenet 과 같은 작은 backbone 모델을 사용하고 ssd 를 사용한다면 충분히 만족할 만한 시간과 성능을 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "### 5. 인원 수, 각도 등 각 문제에 대해서 1 - 4번을 반복해주세요. (정해진 답은 없습니다.)\n",
    "자유롭게 설계해주세요. 각도 문제에 대해서는 아래 방법을 적용해볼 수 있습니다.\n",
    "[Facial Landmark Detection](https://www.learnopencv.com/facial-landmark-detection/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-6. 프로젝트 (5) 칼만 필터 적용하기  \n",
    "### 1.  카메라 앞에서 가만히 있을 때 스티커의 움직임을 관찰해 주세요. 어떤 문제가 발생하나요?\n",
    "가만히 있어도 스티커의 크리가 일정하게 유지되지 않고, 떨리는 것처럼 보이는 현상이 발생합니다.\n",
    "\n",
    "### 2. 이론 강의에서 배운 칼만 필터를 적용해서 스티커 움직임을 안정화시켜 주세요.\n",
    "칼만 필터를 구현한 모듈인 kalman.py와 이를 이용하여 tracker를 구현한 kpkf.py를 이용하여 칼만필터를 적용한 webcam_sticker_kf.py를 함께 첨부합니다. 실행해 보면 현재는 웹캠이 아니라 샘플 동영상에 칼만필터가 적용된 것을 확인하실 수 있습니다.\n",
    "\n",
    "동영상 입력을 웹캠으로 바꾸면 우리가 만들고 있는 웹캠에도 동일하게 적용할 수 있습니다. webcam_sticker_kf.py를 참고하여 자신만의 webcam_sticker.py를 완성해 주세요.\n",
    "\n",
    "수고많으셨습니다. 지금까지 작성하셨던 주피터 노트북 파일과 파이썬 모듈(*.py)를 포함하여 제출해 주세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
