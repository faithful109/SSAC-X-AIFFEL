{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-1. 불안한 시선 이펙트 추가하기  \n",
    "## 16-1. 들어가며  \n",
    "지난번에 이어 눈동자를 검출하는 방법을 다뤄보자.  \n",
    "이번 시간 목표는  \n",
    "  1. 공개 데이터 사용해서 라벨 직접 모아보기\n",
    "  2. 색상 값을 이용한 검출 방법\n",
    "  3. 라벨링 툴 만들기 - point selection\n",
    "  4. 째려보는 효과 구현하기  \n",
    "  \n",
    "### 준비물  \n",
    "-----------------------------------------------\n",
    "프로젝트를 위한 디렉토리를 생성해보자  \n",
    "```\n",
    "$ mkdir -p ~/aiffel/coarse_to_fine/data\n",
    "```\n",
    "파일을 다운받아 디렉토리에서 작업하자  \n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/coarse_to_fine_pjt.zip\n",
    "$ unzip coarse_to_fine_pjt.zip\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-2. 위치 측정을 위한 라벨링 툴 만들기(1) OpenCV 사용  \n",
    "이전 노드에서 True/False를 라벨링하는 도구를 만들었는데, 품질이 좋지 못한 데이터는 직접 입력을 해야 합니다.\n",
    "즉 눈동자 위치를 선택할 수 있는 도구를 만들어보겠습니다.  \n",
    "정확한 곳을 지정하기 위해 마우스를 사용해야 하는데 다행이도 OpenCV에서 마우스 이벤트를 callback 함수로 지원합니다.  \n",
    "[콜백함수](https://satisfactoryplace.tistory.com/18)  \n",
    "**Q1. callback 함수는 무엇인가요?**  \n",
    "ANS) 어떤 이벤트에 의해 호출되어지는 함수, 다른 함수의 인자로 이용되는 함수  \n",
    "OpenCV에서 지원하는 마우스 이벤트 형태는 아래 참고자료를 통해 확인해 보세요.\n",
    "[참고자료](https://opencv-python.readthedocs.io/en/latest/doc/04.drawWithMouse/drawWithMouse.html)  \n",
    "**Q2. OpenCV에서 마우스 이벤트를 확인하고 callback을 호출하는 함수는 무엇인가요?**  \n",
    "ANS)cv2.setMouseCallback()  \n",
    "**Q3. OpenCV에서 마우스 왼쪽 버튼이 눌러졌을 때 on 되는 flag 는 무엇인가요?**  \n",
    "ANS)cv2.EVENT_LBUTTONDOWN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-3. 위치 측정을 위한 라벨링 툴 만들기 (2) 툴 만들기  \n",
    "OpenCV의 마우스 이벤트를 이용해서 라벨링 툴을 만들어보자  \n",
    "아래 코드를 keypoint_using_mouse.py로 저장  \n",
    "아래 코드는 cv2를 주로 이용하며 기존위치를 사용하지 않고 새로 위치를 정하기 때문에 img_path만 불러오고, flg_button은 마우스 이벤트가 발생할 때 사용할 불리언 타입 전역변수.  \n",
    "```\n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters\n",
    "args.add_argument('img_path', type=str, nargs='?', default=None)\n",
    "\n",
    "config = args.parse_args()\n",
    "\n",
    "flg_button = False\n",
    "```  \n",
    "사용할 함수를 만들어보겠습니다.  \n",
    "먼저 img_path가 유효한지 체크하고(img_path에 디렉토리가 입력될때 해당 디렉토리 내의 첫 이미지를 img_path에 입력하고 경로반환) 이미지간 이동할 move()함수도 선업합니다.  \n",
    "```\n",
    "def check_dir():\n",
    "    if config.img_path is None \\\n",
    "        or len(config.img_path) == 0 \\\n",
    "        or config.img_path == '' \\\n",
    "        or os.path.isdir(config.img_path):\n",
    "        root = os.path.realpath('./')\n",
    "        if os.path.isdir(config.img_path):\n",
    "            root = os.path.realpath(config.img_path)\n",
    "        img_list = sorted(glob(join(root, '*.png')))\n",
    "        img_list.extend(sorted(glob(join(root, '*.jpg'))))\n",
    "        config.img_path = img_list[0]\n",
    "\n",
    "    img_dir = os.path.dirname(os.path.realpath(config.img_path))\n",
    "\n",
    "    return img_dir\n",
    "\n",
    "def move(pos, idx, img_list):\n",
    "    if pos == 1:\n",
    "        idx += 1\n",
    "        if idx == len(img_list):\n",
    "            idx = 0\n",
    "    elif pos == -1:\n",
    "        idx -= 1\n",
    "        if idx == -1:\n",
    "            idx = len(img_list) - 1\n",
    "    return idx\n",
    "```\n",
    "마우스 콜백함수 정의  \n",
    "```\n",
    "# Mouse callback function\n",
    "def select_point(event, x,y, flags, param):\n",
    "    global flg_button, gparam # 전역변수 gparam에 img, point정보 저장\n",
    "    img = gparam['img']\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        flg_button = True\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONUP and flg_button == True:\n",
    "        flg_button = False\n",
    "        print (f'({x}, {y}), size:{img.shape}')\n",
    "        gparam['point'] = [x,y] #마우스 왼쪽 누를 때마다 point에 x,y정보를 리스트로 저장\n",
    "```   \n",
    "main 함수인 blend_view()를 구현하자  \n",
    "```\n",
    "def blend_view():\n",
    "    global gparam\n",
    "    gparam = {}\n",
    "    cv2.namedWindow('show', 0)\n",
    "    cv2.resizeWindow('show', 500, 500)\n",
    "\n",
    "    img_dir = check_dir()\n",
    "\n",
    "    fname, ext = os.path.splitext(config.img_path)\n",
    "    img_list = [os.path.basename(x) for x in sorted(glob(join(img_dir,'*%s'%ext)))]\n",
    "\n",
    "    dict_label = {}\n",
    "    dict_label['img_dir'] = img_dir\n",
    "    dict_label['labels'] = {}\n",
    "\n",
    "    json_path = os.getenv('HOME')+'/aiffel/coarse_to_fine/eye_annotation.json'\n",
    "    json_file = open(json_path, 'w', encoding='utf-8')\n",
    "\n",
    "    idx = img_list.index(os.path.basename(config.img_path))\n",
    "    pfname = img_list[idx]\n",
    "    orig = None\n",
    "    local_point = [] # 저장할 point list\n",
    "    while True:\n",
    "        start = cv2.getTickCount()\n",
    "        fname = img_list[idx]\n",
    "                # 파일의 변경이 없거나 이미지가 없을 때, point 를 초기화함\n",
    "        if pfname != fname or orig is None:\n",
    "            orig = cv2.imread(join(img_dir, fname), 1)\n",
    "            gparam['point'] = []\n",
    "            pfname = fname\n",
    "                # 저장할 point(local point) 와 새로 지정한 gparam['point'] 가 변경된 경우,\n",
    "                # local_point 를 업데이트\n",
    "        if local_point != gparam['point']:\n",
    "            orig = cv2.imread(join(img_dir, fname), 1)\n",
    "            local_point = gparam['point']\n",
    "\n",
    "        img_show = orig\n",
    "        gparam['img'] = img_show\n",
    "        cv2.setMouseCallback('show', select_point) # mouse event\n",
    "\n",
    "        if len(local_point) == 2:\n",
    "            img_show = cv2.circle(img_show, tuple(local_point),\n",
    "                                  2, (0,255,0), -1)\n",
    "            dict_label['labels'][fname] = local_point # label 로 저장\n",
    "\n",
    "        time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
    "\n",
    "        if img_show.shape[0] > 300:\n",
    "            cv2.putText(img_show, '%s'%fname, (5,10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (255,255,255))\n",
    "\n",
    "        print (f'[INFO] ({idx+1}/{len(img_list)}) {fname}... time: {time:.3f}ms', end='\\r')\n",
    "\n",
    "        cv2.imshow('show', img_show)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            return -1\n",
    "        if key == ord('n'):\n",
    "            idx = move(1, idx, img_list)\n",
    "        elif key == ord('p'):\n",
    "            idx = move(-1, idx, img_list)\n",
    "        elif key == ord('v'):\n",
    "            print ()\n",
    "            pprint (dict_label)\n",
    "            print ()\n",
    "        elif key == ord('s'):\n",
    "            json.dump(dict_label, json_file, indent=2)\n",
    "            print (f'[INFO] < {json_path} > saved!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    blend_view()\n",
    "```  \n",
    "blend_view함수는 이론노드에서 만든 라벨링 툴의 구조를 비슷하게 구현하되 다른 점 3가지는 아래와 같습니다.  \n",
    "  * 마우스 이벤트를 사용하기 위해 무한루프를 사용해서 gparam을 입력 받을 수 있게 한 것\n",
    "  * 이미지 변경이 없다면 gparam['point'] 를 초기화하지 않을 것\n",
    "  * 이미지 변경이 없더라도 callback 함수에서 gparam 변경이 일어나는 경우는 수정할 것  \n",
    "\n",
    "\n",
    "아래코드로 이미지를 다운받고 코드를 터미널에서 실행해봅시다.  \n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/original_images/1_hZAciQ9.png -O ./data/eye.png\n",
    "$ python keypoint_using_mouse.py ./data/eye.png\n",
    "```\n",
    "눈동자 지점을 클릭한 후 s를 눌러 저장하면 esc를 눌러 프로그램을 종료할 때 ~/aiffel/coarse_to_fine/eye_annotation.json 에 레이블이 저장됩니다.  \n",
    "이제 레이블을 모아서 학습시킬 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-4. 데이터를 모아보자.  \n",
    "라벨링할 초기 데이터를 수집해야 하는데, 직접 촬영하기에는 시간과 노력이 많이 들기에 공개된 데이터를 적극적으로 활용해보자.  \n",
    "우리가 사용할 데이터의 조건은  \n",
    "  1. 눈이 crop 되어 있고 눈동자 위치를 라벨로 가지고 있는 데이터\n",
    "  2. 얼굴 랜드마크(face landmark)를 가지고 있는 데이터  \n",
    "  3. 얼굴 이미지를 가지고 있는 데이터\n",
    "위의 3가지로, 1번째에 해당하는 데이터셋은 [BioID](https://www.bioid.com/facedb/)가 있습니다.  \n",
    "**Q4. BioID는 몇 장의 이미지를 가지고 있나요? 이미지 해상도는 얼마일까요?**  \n",
    "ANS) 1,521장의 gray image, 384x286  \n",
    "**Q5. BioID는 몇 명의 사람으로 구성돼 있나요?**  \n",
    "ANS)23명  \n",
    "BioID는 우리 문제 해결을 위한 데이터셋이지만, 규모가 너무 작기에 다른 데이터셋을 찾을 필요가 있습니다.  \n",
    "따라서 얼굴 랜드마크가 제공되어 눈 부분을 쉽게 크롭할 수 있는 데이터를 생각해봅시다.  \n",
    "\n",
    "### 랜드마크를 제공하는 데이터셋을 찾아보자  \n",
    "------------------------------------------------------  \n",
    "우리는 dlib 패키지의 얼굴 랜드마크를 사용해왔으니, dlib 패키지를 구현하기 위해 사용된 랜드마크가 어떤 데이터셋으로 학습되었는지를 통해 데이터를 찾아볼 수 있습니다.  \n",
    "구글게 dlib face landmark dataset을 치면 [여기](http://dlib.net/face_landmark_detection.py.html)가 나오는데 주석에 어떤 데이터셋을 사용했는지 안내되어 있습니다.  \n",
    "```\n",
    "#   and was trained on the iBUG 300-W face landmark dataset (see\n",
    "#   https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/):\n",
    "```  \n",
    "iBUG 300-W라는 데이터셋으로 학습했다고 하니 이 데이터셋을 이용하는 방법도 유용할 것 같습니다.  \n",
    "이번에는 운이 좋아 데이터셋을 찾을 수 있었지만, 실무에서 내가 원하는 문제와 관련된 도메인 데이터셋이 없기 때무에 3번 방법도 고려해야 합니다.  \n",
    "3번 연습을 위해서 LFW 데이터셋을 사용해봅시다.  \n",
    "이 데이터셋은 안면인식과 관련된 데이터셋으로 얼굴포함된 이미지만 있고, 랜드마크 정보가 없는 데이터셋입니다.  \n",
    "따라서 dlib를 적용해서 랜드마크와 얼굴 위치를 찾고 눈을 크롭한뒤에 라벨링할수 있습니다.  \n",
    "데이터는 [여기](http://vis-www.cs.umass.edu/lfw/lfw.tgz)서 다운로드 가능합니다.  \n",
    "```\n",
    "$ cd ~ && wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
    "$ tar -xvzf lfw.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-5. Mean-shift를 이용한 눈동자 검출 방법 (1) 이론  \n",
    "<img src=\"eye_detect.png\">  \n",
    "쉽게 생각할만한 눈동자 검출 방식은 **\"눈동자는 주변 부분에 비해 어두운 색을 지니고 있다.\"** 를 가정으로 반전된 1D 이미지에서 최댓값을 찾는 방법입니다  \n",
    "<img src=\"eye.png\">  \n",
    "위와 같이 눈동자만 잘 보이면 다행이지만, 아래와 같이 머리카락이 같이 나타나면 눈동자보다 가장자리에 수렴할 확률이 높습니다.  \n",
    "<img src=\"hair.png\">  \n",
    "2차원 블러 특성 이미지(feature image)에서 눈동자가 2차원 정규분포로 나타나는 영역이 있는 것으로 보이고, 1차원 누적 그래프를 봤을 때 x축으로 2개의 봉우리가 있고 최댓값을 찾는 알고리즘을 왼쪽부터 시작했다면 가장 왼쪽에서 만난 255에 수렴할 것이기에 1D 누적 그래프와 2D 특성이미지를 모두 사용합니다.  \n",
    "<img src=\"step.png\">  \n",
    "  **1. 이미지 중심점을 초기값으로 설정하겠습니다.**\n",
    "  눈의 중심에 눈동자가 있을 확률이 높기 때문에 초기값으로 정하기에 아주 좋습니다.)\n",
    "  **2. 중심점을 기준으로 작은 box를 설정합니다.**\n",
    "  box의 크기는 문제에 따라 적절한 값을 설정해야 합니다.\n",
    "  그림에서 회색박스를 생각하시면 됩니다.\n",
    "  **3. box 내부의 pixel 값을 이용해서 '무게중심'을 찾습니다.**\n",
    "  이 때 무게중심은 pixel intensity를 weight 로 사용할 수 있습니다.\n",
    "  **4. 찾은 무게중심을 새로운 box의 중심으로 설정합니다.**\n",
    "  이 단계에서 박스가 이동하게 됩니다. 이제 회색박스에서 초록색박스로 관심영역이 이동했습니다.\n",
    "  **5. 다시 초록색 박스를 기준으로 2-4를 반복합니다.**\n",
    "  **6. 중심점이 수렴할 때 까지 2~5를 반복하면 수렴한 점의 위치로 눈동자를 찾을 수 있습니다.**  \n",
    "\n",
    "위의 방식을 머신러닝의 Mean Shift알고리즘과 비슷하며 mean shift는 탐색반경 내 데이터 포인트의 평균을 구하고 평균 위치로 이동을 반복해 가면서 데이터 분포의 중심으로 이동합니다.[Mean Shift 추적](https://darkpgmr.tistory.com/64)  \n",
    "\n",
    "**Q6. mean shift를 이용해서 global optima를 찾을 수 있는 방법을 설명해주세요.**  \n",
    "ANS) 그런 방법은 존재하지 않는다. mean shift는 local optima에만 수렴하기 때문이다.  \n",
    "**Q7. mean shift 의 단점은 무엇인가요?**  \n",
    "ANS) 초기값에 따라 수렴 위치가 달라진다. 항상 일정한 성능을 보장하기 힘들다.  \n",
    "**Q8. mean shift는 컴퓨터 비전의 어떤 분야에 응용할 수 있나요?**  \n",
    "ANS) Mean Shift는 물체추적(object tracking), 영상 세그멘테이션(segmentation), 데이터 클러스터링(clustering), 경계를 보존하는 영상 스무딩(smoothing) 등 다양하게 활용될 수 있다.  \n",
    "**Q9. 1차원 데이터 분포에 mean shift를 적용하면 어떤 형태를 나타내는지 이야기해 봅시다.**  \n",
    "가우시안 분포에서 등산하듯이 위로 올라가는 형태를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-6. Mean-shift를 이용한 눈동자 검출 방법 (2) 실습  \n",
    "눈동자를 검출하는 mean shift 방법을 코드로 구현. 지난번 노드에서 했던 코드를 eye_center_basic.py에 저장하고 베이스라인으로 사용합니다.  \n",
    "이 코드의 동작은 show_substep argument 옵션을 True로 주게 되면 매 스텝마다 작동을 차례차례 확인해볼수 잇습니다. 옵션을 False로 주거나 생략하면 최종결과만 확인.  \n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine && python eye_center_basic.py True\n",
    "```  \n",
    "이제 mean shift 알고리즘을 적용하여 eye_center_meanshift.py를 생성해보자  \n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine && cp eye_center_basic.py eye_center_meanshift.py\n",
    "```\n",
    "눈 이미지를 low pass filter(bilateral)를 이용해서 smoothing.  \n",
    "다음으로 1차원 값으로 누적시킨 후 y축 기준으로 최댓값을 찾아서 y축의 중심점 좌표를 먼저 얻어냅니다.(y축이 x축에 비해 변화가 적기에 간단히 구현)  \n",
    "x축은 1차원 최댓값 지점을 기준으로 mean shift 수행. 양끝단에 수렴하는 예외를 처리한 후 결과를 출력  \n",
    "결과를 뽑아보자  \n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine && python eye_center_meanshift.py\n",
    "```  \n",
    "여전히 눈동자 중심이 아니어서 아쉽고 기존 머신러닝 알고리즘으로는 큰 성능향상을 기대하기 어려보입니다.  \n",
    "하지만, 예외상황에 좀더 강건한 모델을 만들수 있어서 일반화에 좀더 가까워졌습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-7. 키포인트 검출 딥러닝 모델 만들기 (1) 데이터 확인  \n",
    "더 좋은 성능을 위해서 딥러닝 모델을 만들어보겠습니다.  \n",
    "이번 단계에는 대량의 눈동자 위치 라벨이 필요합니다. 앞에서 만든 coarse dataset또는 annotation한 라벨이 1만개 이상있어야 성능확인이 가능합니다.  \n",
    "이전 단계의 눈동자 검출 방법을 LFW 데이터셋에 적용하여 필요한 데이터셋을 생성.  \n",
    "데이터셋을 생성하는 코드 prepare_eye_dataset.py를 사용하면 사용할 데이터셋을 LFW 데이터셋으로부터 가공,생성  \n",
    "생성된 데이터셋은 ~/lfw/data/train, ~/lfw/data/valid 아래에서 확인  \n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine && python prepare_eye_dataset.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Hub에서 제공하는 pretrained image feature embedding을 가지고 fine tuning을 해보겠습니다.  \n",
    "가지고 있는 데이터를 케라스 ImageDataGenerator 형식으로 읽습니다. 저는 라벨을 image 형태로 저장해 두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23680 23680\n",
      "Found 23680 images belonging to 1 classes.\n",
      "Found 23680 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#train 23712쌍, val 2638쌍.\n",
    "#배치 사이즈인 32의 배수조건만 맞춰주면 된다.\n",
    "import glob\n",
    "import os\n",
    "\n",
    "home_dir = os.getenv('HOME')+'/lfw'\n",
    "list_image = sorted(glob.glob(home_dir+'/data/train/input/img/*.png'))\n",
    "list_label = sorted(glob.glob(home_dir+'/data/train/label/mask/*.png'))\n",
    "print (len(list_image), len(list_label))\n",
    "\n",
    "# 32의 배수를 벗어나는 파일 경로들을 담은 list\n",
    "list_image_out_of_range = list_image[len(list_image) - (len(list_image) % 32):]\n",
    "list_label_out_of_range = list_label[len(list_label) - (len(list_label) % 32):]\n",
    "\n",
    "# 해당 list가 존재한다면, 파일 삭제\n",
    "if list_image_out_of_range:\n",
    "    for path in list_image_out_of_range:\n",
    "        os.remove(path)\n",
    "if list_label_out_of_range:\n",
    "    for path in list_label_out_of_range:\n",
    "        os.remove(path)\n",
    "\n",
    "IMAGE_SHAPE = (80, 120)\n",
    "data_root = home_dir+'/data/train/input'\n",
    "label_root = home_dir+'/data/train/label'\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data = image_generator.flow_from_directory(str(data_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "label_data = label_generator.flow_from_directory(str(label_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드에서는 image_generator, label generator를 학습할 수 있는 입출력 형식으로 편집합니다. 텐서플로우의 제너레이터(generator) 형식을 사용하고 있기 때문에 출력 형식도 맞추어 주겠습니다.  \n",
    "[제너레이터](https://tensorflow.blog/%ED%9A%8C%EC%98%A4%EB%A6%AC%EB%B0%94%EB%9E%8C%EC%9D%84-%ED%83%84-%ED%8C%8C%EC%9D%B4%EC%8D%AC/%EC%A0%9C%EB%84%88%EB%A0%88%EC%9D%B4%ED%84%B0/)  \n",
    "학습 라벨을 만들 때 3개의 점을 label 이미지에 표시했습니다. 눈의 왼쪽 끝점을 1의 값으로, 오른쪽 끝점은 2의 값으로, 가장 중요한 눈 중심(눈동자)는 3으로 인코딩 했습니다. np.where() 함수로 이미지에서 좌표로 복원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_generation(train_generator, label_generator):\n",
    "    h, w = train_generator.target_size\n",
    "    for images, labels in zip(train_generator, label_generator):\n",
    "        images /= 255.\n",
    "        images = images[..., ::-1] # rgb to bgr\n",
    "\n",
    "        list_point_labels = []\n",
    "        for img, label in zip(images, labels):\n",
    "\n",
    "            eye_ls = np.where(label==1) # leftside\n",
    "            eye_rs = np.where(label==2) # rightside\n",
    "            eye_center = np.where(label==3)\n",
    "\n",
    "            lx, ly = [eye_ls[1].mean(), eye_ls[0].mean()]\n",
    "            rx, ry = [eye_rs[1].mean(), eye_rs[0].mean()]\n",
    "            cx, cy = [eye_center[1].mean(), eye_center[0].mean()]\n",
    "\n",
    "            if len(eye_ls[0])==0 or len(eye_ls[1])==0:\n",
    "                lx, ly = [0, 0]\n",
    "            if len(eye_rs[0])==0 or len(eye_rs[1])==0:\n",
    "                rx, ry = [w, h]\n",
    "            if len(eye_center[0])==0 or len(eye_center[1])==0:\n",
    "                cx, cy = [0, 0]\n",
    "\n",
    "            np_point_label = np.array([lx/w,ly/h,rx/w,ry/h,cx/w,cy/h], dtype=np.float32)\n",
    "\n",
    "            list_point_labels.append(np_point_label)\n",
    "        np_point_labels = np.array(list_point_labels)\n",
    "        yield (images, np_point_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10. 좌표로 복원할 때 eye_ls[1].mean() 으로 평균값을 구했습니다. 왜 그랬을까요? 어떤 장점을 활용하려고 한 걸까요?**  \n",
    "ANS) 눈 크기가 이미지마다, 사람마다 다르기 때문에 반드시 resize를 해야 합니다. 이 때 라벨을 이미지에 하나의 점으로 표현하면 resize 과정에서 소실될 수 있습니다. 이런 단점을 극복하기 위해 라벨 이미지를 만들 때 gaussian smoothing을 적용해서 변화에 유연하게 대응 할 수 있도록 했습니다. 이 방법을 취하면 이후 augmentation을 구현할 때도 추가적인 노력없이 바로 라벨을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 120, 3) [0.         0.         1.         1.         0.55833334 0.40823644]\n",
      "(80, 120, 3) [0.         0.         1.         1.         0.22083333 0.28555328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#만들어진 제네레이터로 데이터 포인트를 뽑아 관찰\n",
    "# 120X80의 정해진 크기로 이미지가 잘 출력되고, 라벨또한 0~1값으로 정규화되어있는 것을 확인할 수 있다.\n",
    "user_train_generator = user_generation(image_data, label_data)\n",
    "for i in range(2):\n",
    "    dd = next(user_train_generator)\n",
    "    print (dd[0][0].shape, dd[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-8. 키포인트 검출 딥러닝 모델 만들기 (2) 모델 설계  \n",
    "데이터가 준비되었으니 네트워크 설계.  \n",
    "데이터가 없는 상황이기에 미리 학습된 모델 활용.  \n",
    "TensorFlow Hub의 ResNet 특성추출기부분 백본으로 사용  \n",
    "tf.keras.Sequential()을 이용해서 백본 네트워크와 fully connected layer를 쌓아서 아주 쉽게 모델을 완성할 수 있습니다. 데이터 제너레이터를 만들 때 출력을 6개((x, y) 좌표 2개 * 점 3개) 로 했기 때문에 num_classes 는 6으로 설정합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2048)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 23,577,094\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' tf hub feature_extractor '''\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                            input_shape=(80,120,3))\n",
    "\n",
    "image_batch = next(image_data)\n",
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "feature_extractor_layer.trainable = False\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    #layers.Dense(1024, activation='relu'),\n",
    "    #layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제는 점을 맞는 위치로 추정하는 position regression 문제이기 때문에 loss와 metric을 각각 mse 와 mae 로 설정했습니다. mae 를 통해서 픽셀 위치가 평균적으로 얼마나 차이나는지 확인하면서 학습할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률을 조절하는 함수\n",
    "def lr_step_decay(epoch):\n",
    "      init_lr = 0.0005 #self.flag.initial_learning_rate\n",
    "      lr_decay = 0.5 #self.flag.learning_rate_decay_factor\n",
    "      epoch_per_decay = 2 #self.flag.epoch_per_decay\n",
    "      lrate = init_lr * math.pow(lr_decay, math.floor((1+epoch)/epoch_per_decay))\n",
    "      return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23680 32 740\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740/740 [==============================] - 23s 32ms/step - loss: 0.0210 - mae: 0.0712\n",
      "Epoch 2/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0143 - mae: 0.0563\n",
      "Epoch 3/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0138 - mae: 0.0546\n",
      "Epoch 4/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0133 - mae: 0.0536\n",
      "Epoch 5/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0132 - mae: 0.0531\n",
      "Epoch 6/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0129 - mae: 0.0525\n",
      "Epoch 7/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0129 - mae: 0.0524\n",
      "Epoch 8/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0128 - mae: 0.0522\n",
      "Epoch 9/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0128 - mae: 0.0521\n",
      "Epoch 10/10\n",
      "740/740 [==============================] - 16s 22ms/step - loss: 0.0128 - mae: 0.0521\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = image_data.samples//image_data.batch_size\n",
    "print (image_data.samples, image_data.batch_size, steps_per_epoch)\n",
    "# 23712 32 741 -> 데이터를 batch_size(32) 의 배수로 맞춰 준비해 주세요. \n",
    "\n",
    "assert(image_data.samples % image_data.batch_size == 0)  # 데이터가 32의 배수가 되지 않으면 model.fit()에서 에러가 발생합니다.\n",
    "\n",
    "learning_rate = LearningRateScheduler(lr_step_decay)\n",
    "\n",
    "history = model.fit(user_train_generator, epochs=10,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks = [learning_rate]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-9. 키포인트 검출 딥러닝 모델 만들기 (3) 평가  \n",
    "검증 데이터는 섞어줄 필요가 없기에 shuffle=False옵션추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2646 images belonging to 1 classes.\n",
      "Found 2646 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (80, 120)\n",
    "\n",
    "home_dir = os.getenv('HOME')+'/lfw'\n",
    "\n",
    "val_data_root = home_dir + '/data/val/input'\n",
    "val_label_root = home_dir + '/data/val/label'\n",
    "\n",
    "image_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data_val = image_generator.flow_from_directory(str(val_data_root), class_mode=None, target_size=IMAGE_SHAPE, shuffle=False)\n",
    "label_data_val = label_generator.flow_from_directory(str(val_label_root), class_mode=None, target_size=IMAGE_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-708857ea9b33>:3: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-708857ea9b33>:3: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n",
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012717033736407757 0.05168704688549042\n"
     ]
    }
   ],
   "source": [
    "#제네레이터를 만들고 evaluate_generator()를 수행  \n",
    "user_val_generator = user_generation(image_data_val, label_data_val)\n",
    "mse, mae = model.evaluate_generator(user_val_generator, image_data_val.n // 32)\n",
    "print(mse, mae)\n",
    "# 점 픽셀갯수기준으로 점 픽셀 기준X에러 갯수 개의 픽셀 에러가 나는 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# img test\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(val_data_root+'/img/eye_000010_l.png')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.3800732e-04 1.5259015e-04]\n",
      "  [1.1999979e+02 7.9999916e+01]\n",
      "  [4.8898815e+01 2.4125986e+01]]]\n"
     ]
    }
   ],
   "source": [
    "# 이미지를 120X80으로 resize한 후 배치를 나타낼 수 있는 4차원 텐서로 변경.\n",
    "# 이미지 1장만 출력할 것이니 배치크기 1로 만들자\n",
    "# img test\n",
    "np_inputs = np.expand_dims(cv2.resize(img, (120, 80)), axis=0)\n",
    "preds = model.predict(np_inputs/255., 1)\n",
    "\n",
    "repred = preds.reshape((1, 3, 2))\n",
    "repred[:,:,0] *= 120\n",
    "repred[:,:,1] *= 80\n",
    "print (repred)\n",
    "# 출력결과를 뽑아보면 아래와 같이 나옵니다. 1행부터 좌측, 우측, 중앙 좌표를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[120.  80.]\n",
      "[49. 24.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#결과를 이미지에 출력해 볼까요? pt 값은 120x80 으로 뽑았는데 우리가 사용하는 데이터 크기는 60x40입니다. 따라서 pt 에 0.5 를 곱해서 그림에 출력합니다.\n",
    "#(주의) pt값을 뽑을때의 이미지 크기 기준(120X80)은 고정이지만, 사용하는 데이터의 크기는 매번 달라질 것입니다. 보정치 설정에 유의해 주세요.\n",
    "show = img.copy()\n",
    "for pt in repred[0]:\n",
    "    print (pt.round())\n",
    "    show = cv2.circle(show, tuple((pt*0.5).astype(int)), 3, (0,255,255), -1)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-10. 프로젝트: 카메라 앱에 당황한 표정 효과를 적용해보기  \n",
    "<img src=\"image.jpg\">  \n",
    "그림처럼 놀라서 눈이 튀어나오는 듯 한 효과를 내봅시다. 우선 눈을 찾고, 눈에 효과를 적용해 보세요. 어려워 보여도 차근차근 한다면 충분히 간단한 프로토타입을 구현하실 수 있습니다.  \n",
    "\n",
    "### 1. 이론 시간에 다룬 모델을 참고하여 딥러닝 모델을 설계해 봅시다.  \n",
    "----------------------------------\n",
    "7~9번 스텝에서 키포인트 검출을 위한 딥러닝 모델을 만들어본 바 있습니다. 이를 활용해서 눈 이미지에서 적합한 키포인트를 찾는 딥러닝 모델을 구현해 봅시다. 이 모델의 학습을 위해서는 오늘 다룬 것처럼 데이터를 모아 데이터셋을 구축하는 과정이 함께 진행되어야 할 것입니다.  \n",
    "### 2. 눈동자 효과를 추가해 봅시다.\n",
    "-------------------------------\n",
    "추출된 눈 위치에 위의 당황한 표정의 눈 이미지를 합성해 봅시다. 이렇게 합성된 이미지를 제출하는 것까지가 오늘 프로젝트 과제의 목표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
