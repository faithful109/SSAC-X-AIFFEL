{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  pad_sequence를 통해 문장의 길이를 맞춰준다\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          320000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 323,761\n",
      "Trainable params: 323,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 32)          7200      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 32)          7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 334,673\n",
      "Trainable params: 334,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 32  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46182, 41)\n",
      "(46182,)\n"
     ]
    }
   ],
   "source": [
    "x_val = X_train[:100000]   \n",
    "y_val = y_train[:100000]\n",
    "\n",
    "\n",
    "partial_x_train = X_train[100000:]  \n",
    "partial_y_train = y_train[100000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "91/91 [==============================] - 7s 72ms/step - loss: 0.5958 - accuracy: 0.6772 - val_loss: 0.4084 - val_accuracy: 0.8172\n",
      "Epoch 2/10\n",
      "91/91 [==============================] - 1s 14ms/step - loss: 0.3587 - accuracy: 0.8450 - val_loss: 0.3692 - val_accuracy: 0.8375\n",
      "Epoch 3/10\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.2995 - accuracy: 0.8765 - val_loss: 0.3729 - val_accuracy: 0.8391\n",
      "Epoch 4/10\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.2669 - accuracy: 0.8924 - val_loss: 0.3896 - val_accuracy: 0.8338\n",
      "Epoch 5/10\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.2352 - accuracy: 0.9076 - val_loss: 0.4091 - val_accuracy: 0.8315\n",
      "Epoch 6/10\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.2042 - accuracy: 0.9227 - val_loss: 0.4309 - val_accuracy: 0.8313\n",
      "Epoch 7/10\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.1706 - accuracy: 0.9378 - val_loss: 0.4694 - val_accuracy: 0.8293\n",
      "Epoch 8/10\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.1354 - accuracy: 0.9532 - val_loss: 0.5106 - val_accuracy: 0.8254\n",
      "Epoch 9/10\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.1055 - accuracy: 0.9651 - val_loss: 0.5711 - val_accuracy: 0.8226\n",
      "Epoch 10/10\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.0795 - accuracy: 0.9751 - val_loss: 0.6251 - val_accuracy: 0.8215\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 4s - loss: 0.6412 - accuracy: 0.8162\n",
      "[0.641207218170166, 0.8162418603897095]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, acc, 'ro', label='Training accuracy')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/ko.tsv'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00320347, -0.0675672 ,  0.07626551,  0.09401738,  0.04084894,\n",
       "       -0.04093398, -0.09556052, -0.0067309 ,  0.03242214, -0.09263417,\n",
       "       -0.0127349 ,  0.07013225, -0.03087168,  0.06508426,  0.04058094,\n",
       "       -0.02719566, -0.04432077, -0.06192229, -0.0703969 ,  0.05251942,\n",
       "        0.02013515,  0.03140837, -0.06065933, -0.03219759,  0.01792696,\n",
       "        0.01858612, -0.03652272,  0.06548666, -0.06241552,  0.01180926,\n",
       "       -0.00027937,  0.04360297], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['추천']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('개념', 0.8102073073387146),\n",
       " ('♡♡♡', 0.7993922829627991),\n",
       " ('얼른', 0.7980872988700867),\n",
       " ('흥미진진', 0.7903882265090942),\n",
       " ('안다면', 0.7803696393966675),\n",
       " ('았었', 0.7798968553543091),\n",
       " ('생명력', 0.7791825532913208),\n",
       " ('오래도록', 0.7778271436691284),\n",
       " ('대단히', 0.7777178287506104),\n",
       " ('very', 0.777282178401947)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"사랑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.21245585, -0.76455975,  1.2335763 , -2.0687602 ,  0.11830104,\n",
       "       -0.54881305, -1.1947731 ,  0.3467694 , -1.0311009 , -0.3603907 ,\n",
       "        1.3440485 , -0.4072002 ,  1.1131706 , -0.9551345 , -0.7064215 ,\n",
       "       -0.1686603 , -0.58696073, -1.4859254 ,  0.05223045, -0.23236327,\n",
       "        0.6323806 , -1.1020958 , -1.2635112 ,  0.6220531 ,  0.17838949,\n",
       "       -0.70615005, -1.3628044 , -1.4818763 , -0.18965532,  0.11690602,\n",
       "       -0.09560227, -0.39899632,  1.0731212 , -0.63874424,  0.017969  ,\n",
       "        0.98583263,  1.3941766 ,  0.08089745, -0.81685823, -0.7022943 ,\n",
       "       -0.5008832 , -0.54513174,  0.34584457, -1.7315279 , -0.24291196,\n",
       "        0.6328651 ,  1.4537793 , -0.7011548 ,  0.80850524,  0.74069935,\n",
       "       -0.9272299 , -1.335555  , -1.1335523 ,  3.2529519 ,  0.6823866 ,\n",
       "       -1.4942434 ,  3.213415  ,  1.0216335 , -0.4152105 , -0.11520938,\n",
       "       -0.08406533, -0.49404764, -1.7769778 ,  0.19683318,  0.48151487,\n",
       "       -0.67674905, -1.5347718 ,  0.17763563, -0.6901806 , -0.71837485,\n",
       "        0.26211363,  1.3736591 , -0.81411374,  1.5484174 , -0.32827973,\n",
       "       -1.2308834 , -0.24815056, -0.05604961,  2.4168842 , -1.3484128 ,\n",
       "       -1.9251053 ,  0.85578465, -1.8256166 , -0.10344915, -0.10654867,\n",
       "        1.0619285 ,  0.5666331 , -0.44050282,  0.8053295 ,  0.10873766,\n",
       "        0.6044379 ,  0.764614  , -0.09914612,  0.88586   ,  0.26618013,\n",
       "        0.36814845,  0.426867  , -1.147402  ,  0.84544444, -0.1843114 ,\n",
       "       -0.7268895 , -0.3212696 ,  0.667064  , -0.58419776,  1.8145548 ,\n",
       "        1.9355031 , -1.3043815 ,  0.04326026,  0.45568457, -1.8683544 ,\n",
       "        0.8834185 , -0.51768565, -1.7198668 , -0.05112309,  2.3985045 ,\n",
       "       -2.3366795 , -0.2968566 , -0.81674945,  1.4297746 ,  0.02107116,\n",
       "        0.9217806 , -2.5649145 , -0.17076743, -1.2345508 ,  0.48951107,\n",
       "       -0.01503422,  1.742437  , -0.3975787 ,  0.15941875,  2.065955  ,\n",
       "        2.0112004 ,  0.430826  ,  1.4105109 ,  1.9929649 ,  0.2523577 ,\n",
       "        2.1973937 ,  1.4095    , -1.4678651 ,  0.05385794, -0.9562361 ,\n",
       "        0.8480321 , -0.11160672, -0.20669545,  0.45160684, -1.8831215 ,\n",
       "       -0.08634222,  1.2188671 , -0.26115584, -0.3497346 , -1.6032007 ,\n",
       "        1.5190088 ,  2.0292878 , -0.29313686, -0.84542555, -0.30137303,\n",
       "        1.0734961 , -0.21645601,  0.10950008, -0.528668  , -0.27094576,\n",
       "        1.2831593 ,  0.5645215 , -1.1138439 ,  0.49422115, -0.66331106,\n",
       "       -1.9084527 ,  1.0288999 ,  0.35668066,  0.5310273 ,  1.5287439 ,\n",
       "       -0.8891878 ,  0.9435555 , -0.28362617, -1.3905132 , -1.3402418 ,\n",
       "       -0.16271193, -1.6028023 , -1.3273442 , -0.49335095,  0.84329164,\n",
       "        1.390656  , -0.03553885, -0.9085696 , -0.6598932 ,  1.4210014 ,\n",
       "       -1.78825   ,  1.2472417 , -1.2535207 ,  0.58834684, -1.1036639 ,\n",
       "        0.44645292,  0.6652011 ,  0.92176735, -1.6156495 ,  0.92709404,\n",
       "        1.8791242 , -0.78659415,  0.33630264,  0.5729397 , -1.8942915 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/ko.bin'\n",
    "word2vec = Word2Vec.load(word2vec_path)\n",
    "vector = word2vec['추천']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('슬픔', 0.7216663360595703),\n",
       " ('행복', 0.6759077310562134),\n",
       " ('절망', 0.6468985676765442),\n",
       " ('기쁨', 0.6458414793014526),\n",
       " ('이별', 0.6334798336029053),\n",
       " ('추억', 0.6320937871932983),\n",
       " ('인생', 0.6216273307800293),\n",
       " ('애정', 0.6206068992614746),\n",
       " ('연인', 0.6186063289642334),\n",
       " ('유혹', 0.5965287685394287)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"사랑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/home/ssac19/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:100000]   \n",
    "y_val = y_train[:100000]\n",
    "\n",
    "\n",
    "partial_x_train = X_train[100000:]  \n",
    "partial_y_train = y_train[100000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 40000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 50  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(8, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(8, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, acc, 'ro', label='Training accuracy')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46182, 41)\n",
      "(46182,)\n"
     ]
    }
   ],
   "source": [
    "x_val = X_train[:100000]   \n",
    "y_val = y_train[:100000]\n",
    "\n",
    "\n",
    "partial_x_train = X_train[100000:]  \n",
    "partial_y_train = y_train[100000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,365,825\n",
      "Trainable params: 2,365,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, word_vector_dim))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1949/1950 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8305WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1950/1950 [==============================] - 37s 19ms/step - loss: 0.3764 - accuracy: 0.8305 - val_loss: 0.3242 - val_accuracy: 0.8599\n",
      "Epoch 2/15\n",
      "1948/1950 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8768WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1950/1950 [==============================] - 37s 19ms/step - loss: 0.2896 - accuracy: 0.8768 - val_loss: 0.3107 - val_accuracy: 0.8676\n",
      "Epoch 3/15\n",
      "1948/1950 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.8966WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1950/1950 [==============================] - 36s 19ms/step - loss: 0.2484 - accuracy: 0.8967 - val_loss: 0.3368 - val_accuracy: 0.8653\n",
      "Epoch 4/15\n",
      "1948/1950 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9133WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1950/1950 [==============================] - 37s 19ms/step - loss: 0.2125 - accuracy: 0.9133 - val_loss: 0.3400 - val_accuracy: 0.8621\n",
      "Epoch 5/15\n",
      "1948/1950 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9284WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1950/1950 [==============================] - 36s 19ms/step - loss: 0.1777 - accuracy: 0.9285 - val_loss: 0.3630 - val_accuracy: 0.8602\n",
      "Epoch 6/15\n",
      "1947/1950 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.9437WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1950/1950 [==============================] - 37s 19ms/step - loss: 0.1448 - accuracy: 0.9437 - val_loss: 0.4309 - val_accuracy: 0.8544\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 4s - loss: 0.4353 - accuracy: 0.8503\n",
      "[0.43533191084861755, 0.8503366708755493]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
